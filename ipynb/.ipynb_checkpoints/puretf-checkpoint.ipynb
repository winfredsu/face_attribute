{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURE-TF (1.13.0rc2) FaceAttribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((?, 192, 160, 3), (?, 40)), types: (tf.int8, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "NUM_IMGS = 202599\n",
    "\n",
    "def load_labels(labels_path):\n",
    "    f = open(labels_path)\n",
    "    # line 1: number of images\n",
    "    num_imgs = int(f.readline())\n",
    "    # line 2: attribute names, 40 in total\n",
    "    attr_names = f.readline().split()\n",
    "    # line 3 to end: 00xx.jpg -1 1 -1 1 ...\n",
    "    labels = []\n",
    "    for i in range(num_imgs):\n",
    "        labels.append(list(map(np.float32, f.readline().split()[1:])))\n",
    "    labels = np.array(labels)\n",
    "    labels[labels<0] = 0\n",
    "    return labels\n",
    "\n",
    "def load_imgs(imgs_dir):\n",
    "    img_paths = os.listdir(imgs_dir)\n",
    "    img_paths.sort()\n",
    "    for i in range(len(img_paths)):\n",
    "        img_paths[i] = os.path.join(imgs_dir,img_paths[i])\n",
    "    return img_paths\n",
    "\n",
    "def preprocess(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # uint8 range: [0,255]\n",
    "    img = tf.image.resize(img, [192, 160])\n",
    "    # new range: [-128,127]\n",
    "    img -= 128\n",
    "    img = tf.image.convert_image_dtype(img, tf.int8, saturate=True)\n",
    "    return img    \n",
    "\n",
    "def parse(x):\n",
    "    result = tf.parse_tensor(x, out_type=tf.int8)\n",
    "    result = tf.reshape(result, (192,160,3))\n",
    "    return result\n",
    "\n",
    "if os.path.exists('../dataset/tfrec') == False:\n",
    "    os.mkdir('../dataset/tfrec')\n",
    "    imgs_dir  = '../dataset/img_align_celeba'\n",
    "    img_paths = load_imgs(imgs_dir)\n",
    "    ds_imgs = tf.data.Dataset.from_tensor_slices(img_paths).map(preprocess)\n",
    "    ds_imgs_serialized = ds_imgs.map(tf.serialize_tensor)\n",
    "    tfrec = tf.data.experimental.TFRecordWriter('../dataset/tfrec/imgs.tfrec')\n",
    "    tfrec.write(ds_imgs_serialized)\n",
    "else: \n",
    "    ds_imgs = tf.data.TFRecordDataset('../dataset/tfrec/imgs.tfrec')\n",
    "    ds_imgs = ds_imgs.map(parse, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "labels_path = '../dataset/list_attr_celeba.txt'\n",
    "ds_labels = tf.data.Dataset.from_tensor_slices(load_labels(labels_path))\n",
    "\n",
    "ds_celeba = tf.data.Dataset.zip((ds_imgs, ds_labels))\n",
    "ds_celeba = ds_celeba.apply(tf.data.experimental.shuffle_and_repeat(buffer_size=8192*4))\n",
    "ds_celeba = ds_celeba.batch(32).prefetch(AUTOTUNE)\n",
    "print(ds_celeba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train with Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnet = tf.keras.applications.mobilenet.MobileNet(input_shape=(192,160,3),alpha=0.5,include_top=False,weights=None,pooling='avg')\n",
    "mnet = tf.keras.Sequential([mnet,tf.keras.layers.Dense(40,activation='sigmoid',name='top_dense')], name='mnet_050_faceattr')\n",
    "mnet.summary()\n",
    "\n",
    "mnet.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['binary_accuracy'])\n",
    "\n",
    "mnet.fit(ds_celeba, epochs=5, steps_per_epoch=20, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train with pure TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build a mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobilenet_v1(tensor_in, num_classes, depth_multiplier, is_training):\n",
    "    \"\"\"\n",
    "    Constructs a Mobilenet V1 base convnet\n",
    "    \n",
    "    Args:\n",
    "        tensor_in: a tensor of shape [NHWC]\n",
    "        num_classes: number of channels of the final dense layer\n",
    "        depth_multiplier: multiplier for number of channels, \n",
    "            should be 0.25, 0.5, 0.75 or 1.0\n",
    "        is_training: the model is constructed for training or not\n",
    "        \n",
    "    Returns:\n",
    "        logits: output tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # list of dicts specifying the base net architecture\n",
    "    MOBILENET_V1_BASE_DEFS = [\n",
    "        {'layer':'conv2d', 'name':'Conv_0',  'stride':2, 'depth':32  },\n",
    "        {'layer':'convds', 'name':'Conv_1',  'stride':1, 'depth':64  },\n",
    "        {'layer':'convds', 'name':'Conv_2',  'stride':2, 'depth':128 },\n",
    "        {'layer':'convds', 'name':'Conv_3',  'stride':1, 'depth':128 },\n",
    "        {'layer':'convds', 'name':'Conv_4',  'stride':2, 'depth':256 },\n",
    "        {'layer':'convds', 'name':'Conv_5',  'stride':1, 'depth':256 },\n",
    "        {'layer':'convds', 'name':'Conv_6',  'stride':2, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_7',  'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_8',  'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_9',  'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_10', 'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_11', 'stride':1, 'depth':512 },\n",
    "        {'layer':'convds', 'name':'Conv_12', 'stride':2, 'depth':1024},\n",
    "        {'layer':'convds', 'name': 'Conv_13', 'stride':1, 'depth':1024}\n",
    "    ]\n",
    "    \n",
    "    # hyperparams to use\n",
    "    activation_fn = tf.nn.relu6\n",
    "    normalizer_fn=tf.contrib.slim.batch_norm\n",
    "    normalizer_params = {\n",
    "        'is_training': is_training,\n",
    "        'center': True, \n",
    "        'scale': True, \n",
    "        'decay': 0.9997, \n",
    "        'epsilon': 0.001, \n",
    "        'updates_collections': tf.GraphKeys.UPDATE_OPS\n",
    "    }\n",
    "    weights_initializer = tf.truncated_normal_initializer(stddev=0.09)\n",
    "    weights_regularizer = tf.contrib.layers.l2_regularizer(0.00004)\n",
    "    \n",
    "    with tf.variable_scope('MobilenetV1', [tensor_in]):\n",
    "        net = tensor_in\n",
    "        # conv layers\n",
    "        for layer_def in MOBILENET_V1_BASE_DEFS:\n",
    "            if layer_def['layer']=='conv2d':\n",
    "                net = tf.contrib.slim.conv2d( net,\n",
    "                                              num_outputs=layer_def['depth']*depth_multiplier,\n",
    "                                              kernel_size=[3,3],\n",
    "                                              stride=layer_def['stride'],\n",
    "                                              activation_fn=activation_fn,\n",
    "                                              normalizer_fn=normalizer_fn,\n",
    "                                              normalizer_params=normalizer_params,\n",
    "                                              weights_initializer=weights_initializer,\n",
    "                                              weights_regularizer=weights_regularizer,\n",
    "                                              scope=layer_def['name'])\n",
    "            elif layer_def['layer'] == 'convds':\n",
    "                # depthwise conv\n",
    "                net = tf.contrib.slim.separable_conv2d(net, \n",
    "                                                       num_outputs=None, # to skip pointwise stage\n",
    "                                                       kernel_size=[3,3], \n",
    "                                                       stride=layer_def['stride'], \n",
    "                                                       activation_fn=activation_fn,\n",
    "                                                       normalizer_fn=normalizer_fn,\n",
    "                                                       normalizer_params=normalizer_params,\n",
    "                                                       weights_initializer=weights_initializer,\n",
    "                                                       scope=layer_def['name']+'_depthwise')\n",
    "                # pointwise conv\n",
    "                net = tf.contrib.slim.conv2d(net,\n",
    "                                             num_outputs=layer_def['depth']*depth_multiplier,\n",
    "                                             kernel_size=[1,1],\n",
    "                                             activation_fn=activation_fn,\n",
    "                                             normalizer_fn=normalizer_fn,\n",
    "                                             normalizer_params=normalizer_params,\n",
    "                                             weights_initializer=weights_initializer,\n",
    "                                             weights_regularizer=weights_regularizer,\n",
    "                                             scope=layer_def['name']+'_pointwise')\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('Unsupported layer type'+layer_def['layer'])\n",
    "            \n",
    "        # top layers\n",
    "        convout_shape = net.get_shape().as_list()\n",
    "        net = tf.contrib.slim.avg_pool2d(net, [convout_shape[1],convout_shape[2]], padding='VALID', scope='AvgPool')\n",
    "        net = tf.contrib.slim.dropout(net, keep_prob=0.999, is_training=is_training, scope='Dropout')\n",
    "        logits = tf.contrib.slim.conv2d(net, \n",
    "                                num_outputs=num_classes, \n",
    "                                kernel_size=[1,1], \n",
    "                                activation_fn=None,\n",
    "                                normalizer_fn=None, \n",
    "                                scope='Dense')\n",
    "        logits = tf.squeeze(logits, axis=[1,2], name='Squeeze')\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Train-Eval Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/winfred/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/winfred/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/winfred/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_0/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_1_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_1_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_2_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_2_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_3_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_3_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_4_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_4_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_5_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_5_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_6_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_6_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_7_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_7_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_8_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_8_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_9_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_9_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_10_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_10_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_11_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_11_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_12_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_12_pointwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_13_depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after MobilenetV1/Conv_13_pointwise/add_fold\n"
     ]
    }
   ],
   "source": [
    "def train_eval_loop():\n",
    "    # create a session\n",
    "    sess = tf.InteractiveSession()\n",
    "    g = tf.get_default_graph()\n",
    "\n",
    "    # create placeholders for images and labels\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=[None, 192, 160, 3], name='images')\n",
    "    labels_placeholder = tf.placeholder(tf.float32, shape=[None, 40], name='labels')\n",
    "\n",
    "    # create model\n",
    "    logits = mobilenet_v1(images_placeholder, num_classes=40, depth_multiplier=0.5, is_training=True)\n",
    "\n",
    "    # define loss\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels_placeholder, logits)\n",
    "\n",
    "    # create quantized training graph\n",
    "    tf.contrib.quantize.create_training_graph(quant_delay=0)\n",
    "\n",
    "    # learning rate\n",
    "    global_step = tf.Variable(tf.constant(0))\n",
    "    learning_rate = tf.train.exponential_decay(0.01, global_step, 450, 0.8, staircase=True)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_eval_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
